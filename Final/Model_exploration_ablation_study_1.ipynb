{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rent the Runway Data Sample: {'fit': 'fit', 'user_id': '420272', 'bust size': '34d', 'item_id': '2260466', 'weight': '137lbs', 'rating': '10', 'rented for': 'vacation', 'review_text': \"An adorable romper! Belt and zipper were a little hard to navigate in a full day of wear/bathroom use, but that's to be expected. Wish it had pockets, but other than that-- absolutely perfect! I got a million compliments.\", 'body type': 'hourglass', 'review_summary': 'So many compliments!', 'category': 'romper', 'height': '5\\' 8\"', 'size': 14, 'age': '28', 'review_date': 'April 20, 2016'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# File paths\n",
    "file1 = \"renttherunway_final_data.json\"\n",
    "\n",
    "# Function to read JSON lines data from a file\n",
    "def read_json_lines(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.strip()))  # Load each JSON object per line\n",
    "    return data\n",
    "\n",
    "# Load data from each file\n",
    "renttherunway_data = read_json_lines(file1)\n",
    "\n",
    "# Display the data (printing just the first entry for readability)\n",
    "print(\"Rent the Runway Data Sample:\", renttherunway_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "192544"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(renttherunway_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>192544.000000</td>\n",
       "      <td>1.925440e+05</td>\n",
       "      <td>192462.000000</td>\n",
       "      <td>192544.000000</td>\n",
       "      <td>191584.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>499494.100149</td>\n",
       "      <td>1.045684e+06</td>\n",
       "      <td>9.092371</td>\n",
       "      <td>12.245175</td>\n",
       "      <td>33.871017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>289059.719328</td>\n",
       "      <td>8.053148e+05</td>\n",
       "      <td>1.430044</td>\n",
       "      <td>8.494877</td>\n",
       "      <td>8.058083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.233730e+05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>250654.250000</td>\n",
       "      <td>1.950760e+05</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499419.000000</td>\n",
       "      <td>9.483960e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>750974.000000</td>\n",
       "      <td>1.678888e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999997.000000</td>\n",
       "      <td>2.966087e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id       item_id         rating           size  \\\n",
       "count  192544.000000  1.925440e+05  192462.000000  192544.000000   \n",
       "mean   499494.100149  1.045684e+06       9.092371      12.245175   \n",
       "std    289059.719328  8.053148e+05       1.430044       8.494877   \n",
       "min         9.000000  1.233730e+05       2.000000       0.000000   \n",
       "25%    250654.250000  1.950760e+05       8.000000       8.000000   \n",
       "50%    499419.000000  9.483960e+05      10.000000      12.000000   \n",
       "75%    750974.000000  1.678888e+06      10.000000      16.000000   \n",
       "max    999997.000000  2.966087e+06      10.000000      58.000000   \n",
       "\n",
       "                 age  \n",
       "count  191584.000000  \n",
       "mean       33.871017  \n",
       "std         8.058083  \n",
       "min         0.000000  \n",
       "25%        29.000000  \n",
       "50%        32.000000  \n",
       "75%        37.000000  \n",
       "max       117.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "renttherunway_df = pd.read_json(file1, lines=True)\n",
    "renttherunway_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Null values in Renttherunway dataset: \n",
      "fit                   0\n",
      "user_id               0\n",
      "bust size         18411\n",
      "item_id               0\n",
      "weight            29982\n",
      "rating               82\n",
      "rented for           10\n",
      "review_text           0\n",
      "body type         14637\n",
      "review_summary        0\n",
      "category              0\n",
      "height              677\n",
      "size                  0\n",
      "age                 960\n",
      "review_date           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(f\" Null values in Renttherunway dataset: \\n{renttherunway_df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Null values in Renttherunway dataset after cleaning:\n",
      "fit               0\n",
      "user_id           0\n",
      "bust size         0\n",
      "item_id           0\n",
      "weight            0\n",
      "rating            0\n",
      "rented for        0\n",
      "review_text       0\n",
      "body type         0\n",
      "review_summary    0\n",
      "category          0\n",
      "height            0\n",
      "size              0\n",
      "age               0\n",
      "review_date       0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>size</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>146381.000000</td>\n",
       "      <td>1.463810e+05</td>\n",
       "      <td>146381.000000</td>\n",
       "      <td>146381.000000</td>\n",
       "      <td>146381.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>498991.574528</td>\n",
       "      <td>1.052277e+06</td>\n",
       "      <td>9.081985</td>\n",
       "      <td>11.437919</td>\n",
       "      <td>34.089800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>289658.524623</td>\n",
       "      <td>8.091076e+05</td>\n",
       "      <td>1.437853</td>\n",
       "      <td>7.826784</td>\n",
       "      <td>8.113217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>1.233730e+05</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>249294.000000</td>\n",
       "      <td>1.956130e+05</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>29.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>499034.000000</td>\n",
       "      <td>9.618190e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>32.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>750840.000000</td>\n",
       "      <td>1.687082e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>37.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>999997.000000</td>\n",
       "      <td>2.966087e+06</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>117.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             user_id       item_id         rating           size  \\\n",
       "count  146381.000000  1.463810e+05  146381.000000  146381.000000   \n",
       "mean   498991.574528  1.052277e+06       9.081985      11.437919   \n",
       "std    289658.524623  8.091076e+05       1.437853       7.826784   \n",
       "min         9.000000  1.233730e+05       2.000000       0.000000   \n",
       "25%    249294.000000  1.956130e+05       8.000000       4.000000   \n",
       "50%    499034.000000  9.618190e+05      10.000000       9.000000   \n",
       "75%    750840.000000  1.687082e+06      10.000000      16.000000   \n",
       "max    999997.000000  2.966087e+06      10.000000      58.000000   \n",
       "\n",
       "                 age  \n",
       "count  146381.000000  \n",
       "mean       34.089800  \n",
       "std         8.113217  \n",
       "min         0.000000  \n",
       "25%        29.000000  \n",
       "50%        32.000000  \n",
       "75%        37.000000  \n",
       "max       117.000000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove rows with any null values\n",
    "cleaned_renttherunway_df = renttherunway_df.dropna()\n",
    "\n",
    "# Display null values after cleaning to verify\n",
    "cleaned_null_values = cleaned_renttherunway_df.isnull().sum()\n",
    "print(f\"\\nNull values in Renttherunway dataset after cleaning:\\n{cleaned_null_values}\")\n",
    "\n",
    "# Summary of cleaned dataset\n",
    "cleaned_summary = cleaned_renttherunway_df.describe()\n",
    "cleaned_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fit  user_id bust size  item_id  weight  rating     rented for  \\\n",
      "0  fit   420272       34d  2260466  137lbs    10.0       vacation   \n",
      "1  fit   273551       34b   153475  132lbs    10.0          other   \n",
      "3  fit   909926       34c   126335  135lbs     8.0  formal affair   \n",
      "4  fit   151944       34b   616682  145lbs    10.0        wedding   \n",
      "5  fit   734848       32b   364092  138lbs     8.0           date   \n",
      "\n",
      "                                         review_text          body type  \\\n",
      "0  An adorable romper! Belt and zipper were a lit...          hourglass   \n",
      "1  I rented this dress for a photo shoot. The the...  straight & narrow   \n",
      "3  I rented this for my company's black tie award...               pear   \n",
      "4  I have always been petite in my upper body and...           athletic   \n",
      "5  Didn't actually wear it. It fit perfectly. The...           athletic   \n",
      "\n",
      "                                     review_summary  ... height size   age  \\\n",
      "0                              So many compliments!  ...  5' 8\"   14  28.0   \n",
      "1                           I felt so glamourous!!!  ...  5' 6\"   12  36.0   \n",
      "3  Dress arrived on time and in perfect condition.   ...  5' 5\"    8  34.0   \n",
      "4                   Was in love with this dress !!!  ...  5' 9\"   12  27.0   \n",
      "5                   Traditional with a touch a sass  ...  5' 8\"    8  45.0   \n",
      "\n",
      "          review_date band_size  cup_size  weight_numeric  height_inches  \\\n",
      "0      April 20, 2016      34.0       4.0           137.0             68   \n",
      "1       June 18, 2013      34.0       2.0           132.0             66   \n",
      "3   February 12, 2014      34.0       3.0           135.0             65   \n",
      "4  September 26, 2016      34.0       2.0           145.0             69   \n",
      "5      April 30, 2016      32.0       2.0           138.0             68   \n",
      "\n",
      "   body_type_encoded  rented_for_encoded  \n",
      "0                  0                   0  \n",
      "1                  1                   1  \n",
      "3                  2                   2  \n",
      "4                  3                   3  \n",
      "5                  3                   4  \n",
      "\n",
      "[5 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "##### Transform the nan-numerical features to the numerical label.\n",
    "\n",
    "import re\n",
    "def preprocess_data(df):\n",
    "    # Convert bust size (e.g., '34D') into numerical features\n",
    "    def parse_bust_size(bust):\n",
    "        if isinstance(bust, str) and re.match(r'^\\d+[A-Z]$', bust.upper()):  # Validate format like '34D'\n",
    "            bust = bust.upper()  # Normalize to uppercase\n",
    "            band, cup = int(bust[:-1]), bust[-1]\n",
    "            cup_size = ord(cup) - ord('A') + 1  # A=1, B=2, C=3, ...\n",
    "            return band, cup_size\n",
    "        return None, None\n",
    "\n",
    "    df['band_size'], df['cup_size'] = zip(*df['bust size'].apply(parse_bust_size))\n",
    "\n",
    "    # Convert weight (e.g., '137lbs') to numerical\n",
    "    df['weight_numeric'] = df['weight'].str.extract(r'(\\d+)').astype(float)  # Extract numeric part\n",
    "\n",
    "    # Convert height (e.g., '5\\' 8\"') to inches\n",
    "    def height_to_inches(height):\n",
    "        if isinstance(height, str) and re.match(r'^\\d+\\' \\d+\"$', height):\n",
    "            height = height.replace('\"', '').replace(\"'\", \"\")  # Remove double quotes and apostrophe\n",
    "            feet, inches = map(int, height.split())\n",
    "            return feet * 12 + inches\n",
    "        return None\n",
    "\n",
    "    df['height_inches'] = df['height'].apply(height_to_inches)\n",
    "\n",
    "    # Encode body type\n",
    "    body_type_mapping = {v: i for i, v in enumerate(df['body type'].dropna().unique())}\n",
    "    df['body_type_encoded'] = df['body type'].map(body_type_mapping)\n",
    "\n",
    "    # Encode rented for\n",
    "    rented_for_mapping = {v: i for i, v in enumerate(df['rented for'].dropna().unique())}\n",
    "    df['rented_for_encoded'] = df['rented for'].map(rented_for_mapping)\n",
    "\n",
    "    # Drop or fill missing values\n",
    "    df = df.dropna(subset=['band_size', 'cup_size', 'weight_numeric', 'height_inches', \n",
    "                           'body_type_encoded', 'rented_for_encoded', 'fit'])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply preprocessing to the dataset\n",
    "cleaned_df = preprocess_data(cleaned_renttherunway_df)\n",
    "\n",
    "# Check the updated dataset\n",
    "print(cleaned_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (37404, 21)\n",
      "Validate dataset shape: (8020, 21)\n",
      "Test dataset shape: (8002, 21)\n",
      "Label Distribution Summary:\n",
      "       Train (%)  Validate (%)   Test (%)\n",
      "fit    33.140841      33.19202  32.716821\n",
      "large  33.140841      33.19202  32.716821\n",
      "small  33.718319      33.61596  34.566358\n"
     ]
    }
   ],
   "source": [
    "##### Ablation study about sample the fit label to make fit, small, large labels evenly distibuted.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "random_seed = 42\n",
    "\n",
    "# Divide the cleaned dataset into train, validate, and test datasets\n",
    "train_df, temp_df = train_test_split(cleaned_df, test_size=0.3, random_state=random_seed)  # 70% train\n",
    "validate_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=random_seed)  # 15% validate, 15% test\n",
    "\n",
    "# Function to undersample the 'fit' category\n",
    "def undersample(df):\n",
    "    # Find the number of samples in the smallest class\n",
    "    min_class_size = min(df['fit'].value_counts())\n",
    "\n",
    "    # Sample from each category\n",
    "    fit_sample = df[df['fit'] == 'fit'].sample(min_class_size, random_state=random_seed)\n",
    "    small_sample = df[df['fit'] == 'small']\n",
    "    large_sample = df[df['fit'] == 'large']\n",
    "\n",
    "    # Combine the samples into a new dataframe\n",
    "    undersampled_df = pd.concat([fit_sample, small_sample, large_sample]).sample(frac=1, random_state=random_seed)  # Shuffle the dataset\n",
    "    return undersampled_df\n",
    "\n",
    "# Apply undersampling to the train, validate, and test datasets\n",
    "train_df = undersample(train_df)\n",
    "validate_df = undersample(validate_df)\n",
    "test_df = undersample(test_df)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(f\"Train dataset shape: {train_df.shape}\")\n",
    "print(f\"Validate dataset shape: {validate_df.shape}\")\n",
    "print(f\"Test dataset shape: {test_df.shape}\")\n",
    "\n",
    "# Distribution of labels (fit, small, large) in the datasets\n",
    "train_label_distribution = train_df['fit'].value_counts(normalize=True) * 100  # Percentage distribution\n",
    "validate_label_distribution = validate_df['fit'].value_counts(normalize=True) * 100\n",
    "test_label_distribution = test_df['fit'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Create a summary table for label distribution\n",
    "label_distribution_summary = pd.DataFrame({\n",
    "    'Train (%)': train_label_distribution,\n",
    "    'Validate (%)': validate_label_distribution,\n",
    "    'Test (%)': test_label_distribution\n",
    "}).fillna(0)  # Fill missing labels with 0\n",
    "\n",
    "# Display the distribution summary\n",
    "print(\"Label Distribution Summary:\")\n",
    "print(label_distribution_summary)\n",
    "\n",
    "# Ensure 'fit' column is encoded for classification tasks\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['fit_encoded'] = label_encoder.fit_transform(train_df['fit'])\n",
    "validate_df['fit_encoded'] = label_encoder.transform(validate_df['fit'])\n",
    "test_df['fit_encoded'] = label_encoder.transform(test_df['fit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F2 Score (Random Guessing): 0.3375\n"
     ]
    }
   ],
   "source": [
    "## Methond 1.1. \n",
    "#  Just randomly guess the labels and test the f2 score on the test dataset.\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import fbeta_score\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "test_y = test_df['fit_encoded']\n",
    "# Generate random predictions with the same class distribution as the test set\n",
    "unique_classes = np.unique(test_y)\n",
    "random_predictions = np.random.choice(unique_classes, size=len(test_y))\n",
    "\n",
    "# Compute the F2 score for the random predictions\n",
    "f2_score_random = fbeta_score(test_y, random_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"Baseline F2 Score (Random Guessing): {f2_score_random:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline F2 Score (Weighted Random Guessing): 0.3414\n"
     ]
    }
   ],
   "source": [
    "## Methond 1.2. \n",
    "#  Randomly guess the labels based on the appeareance on the training dataset.\n",
    "#  and test the f2 score on the test dataset.\n",
    "\n",
    "train_y = train_df['fit_encoded']\n",
    "test_y = test_df['fit_encoded']\n",
    "# Calculate the probabilities of each label based on its appearance in the training dataset\n",
    "label_probabilities = train_y.value_counts(normalize=True)\n",
    "\n",
    "# Generate random predictions based on the calculated probabilities\n",
    "weighted_random_predictions = np.random.choice(\n",
    "    label_probabilities.index, size=len(test_y), p=label_probabilities.values\n",
    ")\n",
    "\n",
    "# Compute the F2 score for the weighted random predictions\n",
    "f2_score_weighted_random = fbeta_score(test_y, weighted_random_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"Baseline F2 Score (Weighted Random Guessing): {f2_score_weighted_random:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for Naïve Bayes: 0.3539\n"
     ]
    }
   ],
   "source": [
    "## Methond 2.  Naïve Bayes\n",
    "# A Naïve Bayes model is a probabilistic classification algorithm based on Bayes' theorem. It assumes that the features are conditionally independent given the class label, which simplifies the computation.\n",
    "\n",
    "# Here's how we can build a Naïve Bayes model for the \"fit feedback\" classification task:\n",
    "\n",
    "# Steps to Implement Naïve Bayes\n",
    "# Choose the Variant:\n",
    "\n",
    "# If the features are continuous (e.g., weight_numeric, height_inches), use Gaussian Naïve Bayes.\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['weight_numeric', 'height_inches', 'band_size', 'cup_size', 'body_type_encoded']\n",
    "# features = ['body_type_encoded']\n",
    "target = 'fit_encoded'\n",
    "\n",
    "# Prepare training and test datasets\n",
    "train_X = train_df[features]\n",
    "train_y = train_df[target]\n",
    "test_X = test_df[features]\n",
    "test_y = test_df[target]\n",
    "\n",
    "# Standardize the continuous features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Initialize and train the Naïve Bayes model\n",
    "nb_model = GaussianNB()\n",
    "nb_model.fit(train_X_scaled, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = nb_model.predict(test_X_scaled)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_nb = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for Naïve Bayes: {f2_score_nb:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score on the test data: 0.4464\n",
      "\n",
      "Most Connected Factors to Fit Feedback:\n",
      "              Feature  Coefficient\n",
      "0      weight_numeric     0.007689\n",
      "3                 age     0.003980\n",
      "6   body_type_encoded    -0.002026\n",
      "1       height_inches    -0.007493\n",
      "4           band_size    -0.011333\n",
      "2                size    -0.025217\n",
      "5            cup_size    -0.041787\n",
      "7  rented_for_encoded    -0.041875\n"
     ]
    }
   ],
   "source": [
    "## Methond 3.1. LogisticRegression model using all eight features\n",
    "# features = ['weight_numeric', 'height_inches', 'size', 'age', 'band_size', 'cup_size', 'body_type_encoded', 'rented_for_encoded']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure 'fit' column is encoded for classification tasks\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['fit_encoded'] = label_encoder.fit_transform(train_df['fit'])\n",
    "validate_df['fit_encoded'] = label_encoder.transform(validate_df['fit'])\n",
    "test_df['fit_encoded'] = label_encoder.transform(test_df['fit'])\n",
    "\n",
    "# Select relevant features (including preprocessed numerical and encoded features)\n",
    "features = ['weight_numeric', 'height_inches', 'size', 'age', \n",
    "            'band_size', 'cup_size', 'body_type_encoded', 'rented_for_encoded']\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_X = train_df[features]\n",
    "train_y = train_df['fit_encoded']\n",
    "test_X = test_df[features]\n",
    "test_y = test_df['fit_encoded']\n",
    "\n",
    "# Fill missing values in features (if any) with the mean\n",
    "train_X = train_X.fillna(train_X.mean())\n",
    "test_X = test_X.fillna(test_X.mean())\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(test_X)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_test = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score on the test data: {f2_score_test:.4f}\")\n",
    "\n",
    "# Identify most important factors based on coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nMost Connected Factors to Fit Feedback:\")\n",
    "print(feature_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features by RFE: ['weight_numeric', 'height_inches', 'size', 'band_size', 'rented_for_encoded']\n",
      "F2 Score on the test data: 0.4408\n",
      "\n",
      "Most Connected Factors to Fit Feedback:\n",
      "              Feature  Coefficient\n",
      "0      weight_numeric     0.122287\n",
      "3           band_size     0.038993\n",
      "1       height_inches     0.020976\n",
      "4  rented_for_encoded    -0.077131\n",
      "2                size    -0.215325\n"
     ]
    }
   ],
   "source": [
    "## Methond 3.2. The LogisticRegression model using the best 5 features.\n",
    "\n",
    "# Purpose:\n",
    "\n",
    "    # Identify the most important features (using RFE).\n",
    "    # Build a simpler model using only those features.\n",
    "    # Evaluate its performance using the F2 score.\n",
    "\n",
    "# Outcome:\n",
    "\n",
    "    # A logistic regression model trained on the top 5 features, achieving a specific F2 score on the test set.\n",
    "    # Insights into which features most strongly influence the \"Fit\" prediction.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Select relevant features (including preprocessed numerical and encoded features)\n",
    "features = ['weight_numeric', 'height_inches', 'size', 'age', \n",
    "            'band_size', 'cup_size', 'body_type_encoded', 'rented_for_encoded']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[features])\n",
    "test_X_scaled = scaler.transform(test_df[features])\n",
    "train_y = train_df['fit_encoded']\n",
    "test_y = test_df['fit_encoded']\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "rfe = RFE(model, n_features_to_select=5)  # Select the top 5 features\n",
    "rfe.fit(train_X_scaled, train_y)\n",
    "\n",
    "# Display selected features\n",
    "selected_features = [features[i] for i in range(len(features)) if rfe.support_[i]]\n",
    "print(f\"Selected features by RFE: {selected_features}\")\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[selected_features])\n",
    "test_X_scaled = scaler.transform(test_df[selected_features])\n",
    "train_y = train_df['fit_encoded']\n",
    "test_y = test_df['fit_encoded']\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(train_X_scaled, train_y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(test_X_scaled)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_test = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score on the test data: {f2_score_test:.4f}\")\n",
    "\n",
    "# Identify most important factors based on coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nMost Connected Factors to Fit Feedback:\")\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score on the test data: 0.4301\n",
      "\n",
      "Most Connected Factors to Fit Feedback:\n",
      "             Feature  Coefficient\n",
      "0     weight_numeric     0.134428\n",
      "3          band_size     0.032943\n",
      "1      height_inches     0.017679\n",
      "5  body_type_encoded     0.006472\n",
      "4           cup_size    -0.032655\n",
      "2               size    -0.208988\n"
     ]
    }
   ],
   "source": [
    "## Methond 3.3. The LogisticRegression model using all body-related features\n",
    "# features = ['weight_numeric', 'height_inches', 'size', 'age', 'band_size', 'cup_size', 'body_type_encoded', 'rented_for_encoded']\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "# Select relevant features (including preprocessed numerical and encoded features)\n",
    "features = ['weight_numeric', 'height_inches', 'size',\n",
    "            'band_size', 'cup_size', 'body_type_encoded']\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_df[features])\n",
    "test_X_scaled = scaler.transform(test_df[features])\n",
    "train_y = train_df['fit_encoded']\n",
    "test_y = test_df['fit_encoded']\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "model.fit(train_X_scaled, train_y)\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = model.predict(test_X_scaled)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_test = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score on the test data: {f2_score_test:.4f}\")\n",
    "\n",
    "# Identify most important factors based on coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Coefficient': model.coef_[0]\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "print(\"\\nMost Connected Factors to Fit Feedback:\")\n",
    "print(feature_importance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for k-NN Model: 0.3915\n"
     ]
    }
   ],
   "source": [
    "## Methond 4.1.  Distance-Based Model (Similarity Matching) Euclidean Distance - KNN \n",
    "# this model using all features.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['weight_numeric', 'height_inches', 'size', 'age', \n",
    "            'band_size', 'cup_size', 'body_type_encoded', 'rented_for_encoded']\n",
    "target = 'fit_encoded'\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_X = train_df[features]\n",
    "train_y = train_df[target]\n",
    "test_X = test_df[features]\n",
    "test_y = test_df[target]\n",
    "\n",
    "# Standardize the features to ensure proper scaling for distance calculations\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Initialize and train the k-NN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')  # Use Euclidean distance as default\n",
    "knn_model.fit(train_X_scaled, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = knn_model.predict(test_X_scaled)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_knn = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for k-NN Model: {f2_score_knn:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for k-NN Model: 0.4056\n"
     ]
    }
   ],
   "source": [
    "## Methond 4.2.  Distance-Based Model (Similarity Matching) Euclidean Distance - KNN\n",
    "# this model using the body related features and the size.\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['weight_numeric', 'height_inches', 'band_size', 'cup_size', 'body_type_encoded', 'size']\n",
    "target = 'fit_encoded'\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_X = train_df[features]\n",
    "train_y = train_df[target]\n",
    "test_X = test_df[features]\n",
    "test_y = test_df[target]\n",
    "\n",
    "# Standardize the features to ensure proper scaling for distance calculations\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Initialize and train the k-NN model\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')  # Use Euclidean distance as default\n",
    "knn_model.fit(train_X_scaled, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = knn_model.predict(test_X_scaled)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_knn = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for k-NN Model: {f2_score_knn:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for SGD-based SVM Model: 0.4206\n"
     ]
    }
   ],
   "source": [
    "## Methond 5. SGD based SVM\n",
    "# this model using body related features.\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['weight_numeric', 'height_inches', 'band_size', 'cup_size', 'body_type_encoded', 'size']\n",
    "target = 'fit_encoded'\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_X = train_df[features]\n",
    "train_y = train_df[target]\n",
    "test_X = test_df[features]\n",
    "test_y = test_df[target]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Initialize and train the SGDClassifier\n",
    "sgd_svm = SGDClassifier(loss=\"hinge\", random_state=42, max_iter=1000, tol=1e-3)\n",
    "sgd_svm.fit(train_X_scaled, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = sgd_svm.predict(test_X_scaled)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_sgd = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for SGD-based SVM Model: {f2_score_sgd:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for Latent-Factor Model: 0.3922\n"
     ]
    }
   ],
   "source": [
    "## Methond 6. For your classification problem (predicting fit feedback: \"Small,\" \"Fit,\" or \"Large\"), \n",
    "# we can use a Matrix Factorization approach to create latent representations and then classify based on these representations.\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['weight_numeric', 'height_inches', 'band_size', 'cup_size', 'body_type_encoded', 'size']\n",
    "target = 'fit_encoded'\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_X = train_df[features]\n",
    "train_y = train_df[target]\n",
    "test_X = test_df[features]\n",
    "test_y = test_df[target]\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "test_X_scaled = scaler.transform(test_X)\n",
    "\n",
    "# Apply Latent Factor Decomposition\n",
    "n_components = 5  # Number of latent factors\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "train_X_latent = svd.fit_transform(train_X_scaled)\n",
    "test_X_latent = svd.transform(test_X_scaled)\n",
    "\n",
    "# Train a Logistic Regression model on the latent features\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(train_X_latent, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = clf.predict(test_X_latent)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_latent = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for Latent-Factor Model: {f2_score_latent:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for XGBoost Model: 0.4650\n"
     ]
    }
   ],
   "source": [
    "## Methond 7. Gradient Boosting Models (e.g., XGBoost, LightGBM, CatBoost)\n",
    "# Why it’s a good fit:\n",
    "# Handles both numerical and categorical data well.\n",
    "# Captures complex interactions between features without requiring much preprocessing.\n",
    "# Works well for classification problems with structured data.\n",
    "# How it works:\n",
    "# Combines weak learners (decision trees) sequentially to improve predictions by minimizing errors from previous iterations.\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Initialize and train the XGBoost model\n",
    "xgb_model = XGBClassifier(random_state=42, n_estimators=100, max_depth=5, learning_rate=0.1)\n",
    "xgb_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = xgb_model.predict(test_X)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_xgb = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for XGBoost Model: {f2_score_xgb:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for Random Forest Model: 0.4585\n"
     ]
    }
   ],
   "source": [
    "## Method 8\n",
    "# Random Forest\n",
    "# Why it’s a good fit:\n",
    "# Handles non-linear relationships well.\n",
    "# Robust to noisy features and less prone to overfitting compared to single decision trees.\n",
    "# How it works:\n",
    "# Creates multiple decision trees on random subsets of data and features, then aggregates their predictions.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
    "rf_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = rf_model.predict(test_X)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_rf = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for Random Forest Model: {f2_score_rf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F2 Score for Decision Tree Model: 0.4549\n"
     ]
    }
   ],
   "source": [
    "## method 9.1 Single Decision Tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# Define the features and target variable\n",
    "features = ['weight_numeric', 'height_inches', 'band_size', 'cup_size', 'body_type_encoded', 'size']\n",
    "target = 'fit_encoded'\n",
    "\n",
    "# Prepare training and testing data\n",
    "train_X = train_df[features]\n",
    "train_y = train_df[target]\n",
    "test_X = test_df[features]\n",
    "test_y = test_df[target]\n",
    "\n",
    "# Initialize and train the Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42, max_depth=10)  # Limit depth to avoid overfitting\n",
    "dt_model.fit(train_X, train_y)\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_predictions = dt_model.predict(test_X)\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_dt = fbeta_score(test_y, test_predictions, beta=2, average='weighted')\n",
    "\n",
    "print(f\"F2 Score for Decision Tree Model: {f2_score_dt:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.8944502981682109\n",
      "Epoch 2, Loss: 0.7625614121619962\n",
      "Epoch 3, Loss: 0.7138131280931143\n",
      "F2 Score for CNN Model: 0.7056\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Define a custom Dataset for text data\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, tokenizer, max_len):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = str(self.reviews[idx])\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            review,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Parameters\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Load the BERT tokenizer (used only for tokenization)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Split data into train and test sets\n",
    "train_reviews, test_reviews, train_labels, test_labels = train_test_split(\n",
    "    train_df['review_text'], train_df['fit_encoded'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TextDataset(train_reviews.tolist(), train_labels.tolist(), tokenizer, MAX_LEN)\n",
    "test_dataset = TextDataset(test_reviews.tolist(), test_labels.tolist(), tokenizer, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Define the CNN model\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, num_classes, vocab_size, embedding_dim=100, num_filters=100, filter_sizes=[3, 4, 5]):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(1, num_filters, (fs, embedding_dim)) for fs in filter_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(filter_sizes) * num_filters, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embedded = self.embedding(input_ids).unsqueeze(1)  # Add a channel dimension\n",
    "        conved = [torch.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "        pooled = [torch.max(c, dim=2)[0] for c in conved]\n",
    "        cat = self.dropout(torch.cat(pooled, dim=1))\n",
    "        return self.fc(cat)\n",
    "\n",
    "# Initialize the CNN model\n",
    "vocab_size = tokenizer.vocab_size\n",
    "num_classes = len(set(train_labels))\n",
    "cnn_model = TextCNN(num_classes=num_classes, vocab_size=vocab_size).to(torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(cnn_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for epoch in range(EPOCHS):\n",
    "    cnn_model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = cnn_model(input_ids)\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluation loop\n",
    "cnn_model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = cnn_model(input_ids)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Compute the F2 score\n",
    "f2_score_cnn = fbeta_score(all_labels, all_preds, beta=2, average='weighted')\n",
    "print(f\"F2 Score for CNN Model: {f2_score_cnn:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
